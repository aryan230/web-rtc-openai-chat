<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>WebRTC OpenAI Chat</title>
    <style>
      .controls {
        margin: 20px;
      }
      .status {
        color: #666;
        margin: 10px 0;
      }
      .active {
        background: #ff4444;
      }
      button {
        margin: 5px;
      }
      #audioVisualizer {
        width: 300px;
        height: 20px;
        border: 1px solid #ccc;
        margin: 10px 0;
      }
      #volumeLevel {
        height: 100%;
        width: 0%;
        background-color: #4caf50;
        transition: width 0.1s;
      }
    </style>
  </head>
  <body>
    <div class="controls">
      <input type="password" id="apiKey" placeholder="Enter OpenAI API Key" />
      <button id="startButton">Start Recording</button>
      <button id="stopButton" disabled>Stop</button>
    </div>
    <div id="status" class="status"></div>
    <audio id="audioPlayer" controls style="display: none"></audio>
    <div id="audioVisualizer">
      <div id="volumeLevel"></div>
    </div>

    <script>
      const startButton = document.getElementById("startButton");
      const stopButton = document.getElementById("stopButton");
      const status = document.getElementById("status");
      const audioPlayer = document.getElementById("audioPlayer");
      const apiKeyInput = document.getElementById("apiKey");
      let volumeLevel = document.getElementById("volumeLevel");
      let lastVolumeLevel = 0;

      let mediaRecorder;
      let audioChunks = [];
      let audioContext;
      let mediaStream;
      let speechDetected = false;
      let isProcessing = false;
      let silenceTimer = null;
      let isSpeaking = false;
      let shouldContinueRecording = false;
      const SILENCE_THRESHOLD = 0.01; // Reduced sensitivity
      const VOICE_START_THRESHOLD = 0.015; // Threshold to start recording
      const SILENCE_DURATION = 1500; // 1.5 seconds
      const MIN_AUDIO_LENGTH = 0.5; // Minimum audio length in seconds
      const MIN_AUDIO_ENERGY = 0.01; // Minimum average audio energy

      async function initAudio() {
        const apiKey = apiKeyInput.value.trim();
        if (!apiKey) {
          status.textContent = "Please enter OpenAI API key";
          return;
        }

        try {
          shouldContinueRecording = true;
          mediaStream = await navigator.mediaDevices.getUserMedia({
            audio: true,
          });
          audioContext = new AudioContext();
          setupMediaRecorder();

          const mediaStreamSource =
            audioContext.createMediaStreamSource(mediaStream);

          // Create processor node for real-time audio processing
          const processor = audioContext.createScriptProcessor(4096, 1, 1);
          mediaStreamSource.connect(processor);
          processor.connect(audioContext.destination);

          // Handle real-time audio processing with VAD
          processor.onaudioprocess = (e) => {
            const inputData = e.inputBuffer.getChannelData(0);
            const volumeLevel = Math.max(...inputData.map(Math.abs));

            // Update visual indicator
            const scaledVolume = Math.min(volumeLevel * 500, 100);
            document.getElementById("volumeLevel").style.width =
              scaledVolume + "%";

            // Implement better voice detection
            if (volumeLevel > VOICE_START_THRESHOLD) {
              if (!isSpeaking) {
                isSpeaking = true;
                status.textContent = "Recording...";
                stopButton.disabled = false;
              }
              if (silenceTimer) {
                clearTimeout(silenceTimer);
                silenceTimer = null;
              }
            } else if (isSpeaking && volumeLevel < SILENCE_THRESHOLD) {
              if (!silenceTimer) {
                silenceTimer = setTimeout(() => {
                  if (mediaRecorder.state === "recording") {
                    mediaRecorder.stop();
                  }
                  silenceTimer = null;
                }, SILENCE_DURATION);
              }
            }

            lastVolumeLevel = volumeLevel;
          };

          mediaRecorder.start();
          startButton.disabled = true;
          stopButton.disabled = false;
          status.textContent = "Listening...";
        } catch (error) {
          status.textContent = "Error initializing audio: " + error.message;
        }
      }

      async function processAudioWithOpenAI(audioBlob, apiKey) {
        try {
          // Convert audio to format accepted by OpenAI
          const formData = new FormData();
          formData.append("file", audioBlob, "audio.wav");
          formData.append("model", "whisper-1");

          // First, transcribe the audio
          const transcriptionResponse = await fetch(
            "https://api.openai.com/v1/audio/transcriptions",
            {
              method: "POST",
              headers: {
                Authorization: `Bearer ${apiKey}`,
              },
              body: formData,
            }
          );

          if (!transcriptionResponse.ok) {
            throw new Error("Transcription failed");
          }

          const transcriptionData = await transcriptionResponse.json();

          // Then, get AI response using Chat API
          const chatResponse = await fetch(
            "https://api.openai.com/v1/chat/completions",
            {
              method: "POST",
              headers: {
                Authorization: `Bearer ${apiKey}`,
                "Content-Type": "application/json",
              },
              body: JSON.stringify({
                model: "gpt-3.5-turbo",
                messages: [
                  {
                    role: "user",
                    content: transcriptionData.text,
                  },
                ],
              }),
            }
          );

          if (!chatResponse.ok) {
            throw new Error("Chat completion failed");
          }

          const chatData = await chatResponse.json();

          // Finally, convert AI response to speech
          const speechResponse = await fetch(
            "https://api.openai.com/v1/audio/speech",
            {
              method: "POST",
              headers: {
                Authorization: `Bearer ${apiKey}`,
                "Content-Type": "application/json",
              },
              body: JSON.stringify({
                model: "tts-1",
                input: chatData.choices[0].message.content,
                voice: "alloy",
              }),
            }
          );

          if (!speechResponse.ok) {
            throw new Error("Speech synthesis failed");
          }

          const audioResponse = await speechResponse.blob();
          audioPlayer.src = URL.createObjectURL(audioResponse);
          audioPlayer.style.display = "block";
          await audioPlayer.play();

          status.textContent = "Response ready!";
        } catch (err) {
          status.textContent = `Error: ${err.message}`;
          console.error(err);
        }
      }

      function stopRecording() {
        shouldContinueRecording = false;
        if (mediaRecorder && mediaRecorder.state !== "inactive") {
          mediaRecorder.stop();
          mediaStream.getTracks().forEach((track) => track.stop());
          if (audioContext) {
            audioContext.close();
          }
          document.getElementById("volumeLevel").style.width = "0%";
          isSpeaking = false;
        }
        startButton.disabled = false;
        stopButton.disabled = true;
        status.textContent = "Recording stopped";
        audioChunks = [];
        mediaRecorder = null;
        mediaStream = null;
        audioContext = null;
      }

      function hasAudioContent(audioChunks) {
        return new Promise((resolve) => {
          const blob = new Blob(audioChunks, { type: "audio/wav" });
          const audioContext = new AudioContext();
          const fileReader = new FileReader();

          fileReader.onload = async function () {
            const audioBuffer = await audioContext.decodeAudioData(
              fileReader.result
            );
            const channelData = audioBuffer.getChannelData(0);

            // Check audio length
            if (audioBuffer.duration < MIN_AUDIO_LENGTH) {
              resolve(false);
              return;
            }

            // Calculate average energy
            const energy =
              channelData.reduce((sum, sample) => sum + Math.abs(sample), 0) /
              channelData.length;
            resolve(energy > MIN_AUDIO_ENERGY);
          };

          fileReader.readAsArrayBuffer(blob);
        });
      }

      function setupMediaRecorder() {
        if (!mediaStream) return;

        mediaRecorder = new MediaRecorder(mediaStream);

        mediaRecorder.ondataavailable = (event) => {
          audioChunks.push(event.data);
        };

        mediaRecorder.onstop = async () => {
          if (audioChunks.length > 0 && !isProcessing) {
            try {
              // Check if audio contains actual speech
              const hasAudio = await hasAudioContent(audioChunks);
              if (!hasAudio) {
                console.log(
                  "No meaningful audio detected, skipping processing"
                );
                audioChunks = [];
                if (shouldContinueRecording && mediaStream) {
                  setupMediaRecorder();
                  mediaRecorder.start();
                }
                return;
              }
              isProcessing = true;
              status.textContent = "Processing...";
              console.log("Processing audio...");
              const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
              await processAudioWithOpenAI(audioBlob, apiKeyInput.value.trim());
            } catch (error) {
              console.error("Processing error:", error);
              status.textContent = "Processing failed: " + error.message;
            } finally {
              audioChunks = [];
              isProcessing = false;
              if (shouldContinueRecording && mediaStream) {
                console.log("Restarting recording...");
                setupMediaRecorder();
                mediaRecorder.start();
                status.textContent = "Listening...";
              }
            }
          }
        };
      }

      startButton.onclick = initAudio;
      stopButton.onclick = stopRecording;

      // Cleanup on page unload
      window.addEventListener("beforeunload", () => {
        stopRecording();
      });
    </script>
  </body>
</html>
